# -*- coding: utf-8 -*-
"""2-bedah.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14vZ2LNeO6kdmfSNhReH-axvuIK_sLjxk

## Scraping Ulasan Aplikasi Melalui Google Play Store
"""

!pip install google-play-scraper

# Mengimpor pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store.
from google_play_scraper import app, reviews, Sort, reviews_all

# Mengambil semua ulasan dari aplikasi whatsapp
# Proses scraping mungkin memerlukan beberapa saat tergantung pada jumlah ulasan yang ada.
scrapreview = reviews_all(
    'com.instagram.android',        # ID paket aplikasi
    lang='id',             # Bahasa ulasan (default: 'en')
    country='id',          # Negara (default: 'us')
    sort=Sort.MOST_RELEVANT, # Urutan ulasan (default: Sort.MOST_RELEVANT)
    count=10             # Jumlah maksimum ulasan yang ingin diambil
)

import csv
with open('review.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Review", "Rating"])
    for review in scrapreview:
        writer.writerow([review['content'], review['score']])
print("Scraping selesai. Data tersimpan dalam file 'review.csv'")

# scrapreview

import pandas as pd

app_reviews_df = pd.DataFrame(scrapreview)
app_reviews_df.shape
app_reviews_df

df = pd.read_csv('/content/review.csv')
df

import pandas as pd  # Pandas untuk manipulasi dan analisis data
pd.options.mode.chained_assignment = None  # Menonaktifkan peringatan chaining
import numpy as np  # NumPy untuk komputasi numerik
seed = 0
np.random.seed(seed)  # Mengatur seed untuk reproduktibilitas
import matplotlib.pyplot as plt  # Matplotlib untuk visualisasi data
import seaborn as sns  # Seaborn untuk visualisasi data statistik, mengatur gaya visualisasi

import datetime as dt  # Manipulasi data waktu dan tanggal
import re  # Modul untuk bekerja dengan ekspresi reguler
import string  # Berisi konstanta string, seperti tanda baca
from nltk.tokenize import word_tokenize  # Tokenisasi teks
from nltk.corpus import stopwords  # Daftar kata-kata berhenti dalam teks

!pip install sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Stemming (penghilangan imbuhan kata) dalam bahasa Indonesia
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Menghapus kata-kata berhenti dalam bahasa Indonesia

from wordcloud import WordCloud  # Membuat visualisasi berbentuk awan kata (word cloud) dari teks

import nltk  # Import pustaka NLTK (Natural Language Toolkit).
nltk.download('punkt')  # Mengunduh dataset yang diperlukan untuk tokenisasi teks.
nltk.download('stopwords')  # Mengunduh dataset yang berisi daftar kata-kata berhenti (stopwords) dalam berbagai bahasa.

# Membuat DataFrame baru (clean_df) dengan menghapus baris yang memiliki nilai yang hilang (NaN) dari app_reviews_df
clean_df = df.dropna()
clean_df

# Menghapus baris duplikat dari DataFrame clean_df
clean_df = clean_df.drop_duplicates()

# Menghitung jumlah baris dan kolom dalam DataFrame clean_df setelah menghapus duplikat
jumlah_ulasan_setelah_hapus_duplikat, jumlah_kolom_setelah_hapus_duplikat = clean_df.shape
print("Jumlah baris setelah menghapus duplikat:", jumlah_ulasan_setelah_hapus_duplikat)
print("Jumlah kolom setelah menghapus duplikat:", jumlah_kolom_setelah_hapus_duplikat)

import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag
    text = re.sub(r'RT[\s]', '', text) # menghapus RT
    text = re.sub(r"http\S+", '', text) # menghapus link
    text = re.sub(r'[0-9]+', '', text) # menghapus angka
    text = re.sub(r'[^\w\s]', '', text) # menghapus karakter selain huruf dan angka

    text = text.replace('\n', ' ') # mengganti baris baru dengan spasi
    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca
    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks
    return text

def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil
    text = text.lower()
    return text

def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token
    text = word_tokenize(text)
    return text

def filteringText(text): # Menghapus stopwords dalam teks
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy"])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata
    # Membuat objek stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()

    # Memecah teks menjadi daftar kata
    words = text.split()

    # Menerapkan stemming pada setiap kata dalam daftar
    stemmed_words = [stemmer.stem(word) for word in words]

    # Menggabungkan kata-kata yang telah distem
    stemmed_text = ' '.join(stemmed_words)

    return stemmed_text

def toSentence(list_words): # Mengubah daftar kata menjadi kalimat
    sentence = ' '.join(word for word in list_words)
    return sentence

slangwords = {"@": "di", "abis": "habis", "wtb": "beli", "masi": "masih", "wts": "jual", "wtt": "tukar", "bgt": "banget", "maks": "maksimal"}
def fix_slangwords(text):
    words = text.split()
    fixed_words = []

    for word in words:
        if word.lower() in slangwords:
            fixed_words.append(slangwords[word.lower()])
        else:
            fixed_words.append(word)

    fixed_text = ' '.join(fixed_words)
    return fixed_text

# Membersihkan teks dan menyimpannya di kolom 'text_clean'
clean_df['text_clean'] = clean_df['Review'].apply(cleaningText)

# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'
clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)

# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'
clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)

# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'
clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)

# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'
clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)

# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'
clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)

df = clean_df.head(10000)
df

from textblob import TextBlob

def analisis_sentimen(text):
    analysis = TextBlob(text)
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity == 0:
        return 'neutral'
    else:
        return 'negative'

df['polarity'] = df['text_akhir'].apply(analisis_sentimen)
df

nltk.download('vader_lexicon')

# Menggunakan VADER
from nltk.sentiment.vader import SentimentIntensityAnalyzer

def analisis_sentimen_vader(text):
    analyzer = SentimentIntensityAnalyzer()
    analysis = analyzer.polarity_scores(text)
    if analysis['compound'] > 0:
        return 'positive'
    elif analysis['compound'] == 0:
        return 'neutral'
    else:
        return 'negative'

df['polarity_vader'] = df['text_akhir'].apply(analisis_sentimen_vader)
df

# import csv
# import requests
# from io import StringIO

# # Membaca data kamus kata-kata positif dari GitHub
# lexicon_positive = dict()

# response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')
# # Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub

# if response.status_code == 200:
#     # Jika permintaan berhasil
#     reader = csv.reader(StringIO(response.text), delimiter=',')
#     # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma

#     for row in reader:
#         # Mengulangi setiap baris dalam file CSV
#         lexicon_positive[row[0]] = int(row[1])
#         # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive
# else:
#     print("Failed to fetch positive lexicon data")

# # Membaca data kamus kata-kata negatif dari GitHub
# lexicon_negative = dict()

# response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')
# # Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub

# if response.status_code == 200:
#     # Jika permintaan berhasil
#     reader = csv.reader(StringIO(response.text), delimiter=',')
#     # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma

#     for row in reader:
#         # Mengulangi setiap baris dalam file CSV
#         lexicon_negative[row[0]] = int(row[1])
#         # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative
# else:
#     print("Failed to fetch negative lexicon data")

# # Fungsi untuk menentukan polaritas sentimen dari tweet

# def sentiment_analysis_lexicon_indonesia(text):
#     #for word in text:

#     score = 0
#     # Inisialisasi skor sentimen ke 0

#     for word in text:
#         # Mengulangi setiap kata dalam teks

#         if (word in lexicon_positive):
#             score = score + lexicon_positive[word]
#             # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen

#     for word in text:
#         # Mengulangi setiap kata dalam teks (sekali lagi)

#         if (word in lexicon_negative):
#             score = score + lexicon_negative[word]
#             # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen

#     polarity=''
#     # Inisialisasi variabel polaritas

#     # buat polarity neutral, positive, negative
#     if score > 1:
#         polarity = 'positive'
#     elif score < -1:
#         polarity = 'negative'
#     else:
#         polarity = 'neutral'

#     return score, polarity
#     # Mengembalikan skor sentimen dan polaritas teks

# results = df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)
# results = list(zip(*results))
# df['polarity_score'] = results[0]
# df['polarity'] = results[1]
# print(df['polarity'].value_counts())

df

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Pisahkan data menjadi fitur (tweet) dan label (sentimen)
X = df['text_akhir']
# y = df['polarity']
y = df['polarity_vader']

# Encode label
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )
X_tfidf = tfidf.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Ektraksi fitur dengan BoW
from sklearn.feature_extraction.text import CountVectorizer

# Membuat objek CountVectorizer
bow = CountVectorizer(max_features=200, min_df=17, max_df=0.8)
X_bow = bow.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_bow.toarray(), columns=bow.get_feature_names_out())

# Ekstraksi fitur dengan metode GloVe
import gensim.downloader as api

# Mengunduh model GloVe
glove_model = api.load('glove-twitter-25')

# Membuat vektor fitur GloVe
X_glove = np.array([np.mean([glove_model[word] for word in document.split() if word in glove_model] or [np.zeros(25)], axis=0) for document in X])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_glove)

# Ektrasi fitur dengan metode Word2Vec
from gensim.models import Word2Vec

# Tokenisasi teks
tokenized_text = [word_tokenize(text) for text in df['text_akhir']]
# Membuat model Word2Vec
word2vec = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=1)

# Membuat vektor fitur Word2Vec
X_word2vec = np.array([np.mean([word2vec.wv[word] for word in document if word in word2vec.wv] or [np.zeros(100)], axis=0) for document in tokenized_text])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_word2vec)

# Ekstraksi fitur dengan metode FastText
from gensim.models import FastText

# Membuat model FastText
fasttext = FastText(sentences=tokenized_text, vector_size=100, window=5, min_count=1, sg=1)

# Membuat vektor fitur FastText
X_fasttext = np.array([np.mean([fasttext.wv[word] for word in document if word in fasttext.wv] or [np.zeros(100)], axis=0) for document in tokenized_text])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_fasttext)

# Ekstraksi fitur dengan metode Doc2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Membuat dokumen berlabel
tagged_data = [TaggedDocument(words=word_tokenize(text), tags=[str(i)]) for i, text in enumerate(df['text_akhir'])]

# Membuat model Doc2Vec
doc2vec = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=100)
doc2vec.build_vocab(tagged_data)
doc2vec.train(tagged_data, total_examples=doc2vec.corpus_count, epochs=doc2vec.epochs)

# Membuat vektor fitur Doc2Vec
X_doc2vec = np.array([doc2vec.infer_vector(word_tokenize(text)) for text in df['text_akhir']])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_doc2vec)

# Menampilkan hasil ekstraksi fitur
features_df

# Bagi data menjadi data latih dan data uji
# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_glove, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_fasttext, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_doc2vec, y, test_size=0.2, random_state=42)

# from imblearn.over_sampling import SMOTE

# # Seimbangkan kelas dengan SMOTE
# smote = SMOTE(random_state=42)
# X_train, y_train = smote.fit_resample(X_train, y_train)

from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score

# Membuat objek model Naive Bayes (Bernoulli Naive Bayes)
naive_bayes = BernoulliNB()

# Melatih model Naive Bayes pada data pelatihan
# naive_bayes.fit(X_train.toarray(), y_train)
naive_bayes.fit(X_train, y_train)

# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_nb = naive_bayes.predict(X_train.toarray())
# y_pred_test_nb = naive_bayes.predict(X_test.toarray())

y_pred_train_nb = naive_bayes.predict(X_train)
y_pred_test_nb = naive_bayes.predict(X_test)

# Evaluasi akurasi model Naive Bayes
accuracy_train_nb = accuracy_score(y_pred_train_nb, y_train)
accuracy_test_nb = accuracy_score(y_pred_test_nb, y_test)

# Menampilkan akurasi
print('Naive Bayes - accuracy_train:', accuracy_train_nb)
print('Naive Bayes - accuracy_test:', accuracy_test_nb)

from sklearn.ensemble import RandomForestClassifier

# Membuat objek model Random Forest
random_forest = RandomForestClassifier()

# Melatih model Random Forest pada data pelatihan
# random_forest.fit(X_train.toarray(), y_train)
random_forest.fit(X_train, y_train)

# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_rf = random_forest.predict(X_train.toarray())
# y_pred_test_rf = random_forest.predict(X_test.toarray())

y_pred_train_rf = random_forest.predict(X_train)
y_pred_test_rf = random_forest.predict(X_test)


# Evaluasi akurasi model Random Forest
accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)
accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)

# Menampilkan akurasi
print('Random Forest - accuracy_train:', accuracy_train_rf)
print('Random Forest - accuracy_test:', accuracy_test_rf)

from sklearn.linear_model import LogisticRegression

# Membuat objek model Logistic Regression
logistic_regression = LogisticRegression(max_iter=1000)

# Melatih model Logistic Regression pada data pelatihan
# logistic_regression.fit(X_train.toarray(), y_train)

logistic_regression.fit(X_train, y_train)

# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_lr = logistic_regression.predict(X_train.toarray())
# y_pred_test_lr = logistic_regression.predict(X_test.toarray())

y_pred_train_lr = logistic_regression.predict(X_train)
y_pred_test_lr = logistic_regression.predict(X_test)

# Evaluasi akurasi model Logistic Regression pada data pelatihan
accuracy_train_lr = accuracy_score(y_pred_train_lr, y_train)

# Evaluasi akurasi model Logistic Regression pada data uji
accuracy_test_lr = accuracy_score(y_pred_test_lr, y_test)

# Menampilkan akurasi
print('Logistic Regression - accuracy_train:', accuracy_train_lr)
print('Logistic Regression - accuracy_test:', accuracy_test_lr)

from sklearn.tree import DecisionTreeClassifier

# Membuat objek model Decision Tree
decision_tree = DecisionTreeClassifier()

# Melatih model Decision Tree pada data pelatihan
# decision_tree.fit(X_train.toarray(), y_train)
decision_tree.fit(X_train, y_train)


# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_dt = decision_tree.predict(X_train.toarray())
# y_pred_test_dt = decision_tree.predict(X_test.toarray())

y_pred_train_dt = decision_tree.predict(X_train)
y_pred_test_dt = decision_tree.predict(X_test)

# Evaluasi akurasi model Decision Tree
accuracy_train_dt = accuracy_score(y_pred_train_dt, y_train)
accuracy_test_dt = accuracy_score(y_pred_test_dt, y_test)

# Menampilkan akurasi
print('Decision Tree - accuracy_train:', accuracy_train_dt)
print('Decision Tree - accuracy_test:', accuracy_test_dt)

# train dengan deep learning menggunakan LSTM tensorflow
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_integers = label_encoder.fit_transform(y)

y_lstm = to_categorical(y_integers, num_classes=3)


# Tokenisasi teks
tokenizer = Tokenizer(num_words=200, oov_token='<OOV>')
tokenizer.fit_on_texts(df['text_akhir'])

# Ubah teks menjadi urutan token
sequences = tokenizer.texts_to_sequences(df['text_akhir'])

# Padding urutan token maxlen rata-rata
maxlen = df['text_akhir'].apply(lambda x: len(x.split())).mean()
padded = pad_sequences(sequences, padding='post', maxlen=int(maxlen))

# Bagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(padded, y_lstm, test_size=0.2, random_state=42)

# Membuat model LSTM
model = Sequential()
model.add(Embedding(1000, 128, input_length=X_train.shape[1]))
model.add(Bidirectional(LSTM(128)))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))

# Kompilasi model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Latih model
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=3)])

# Evaluasi model
loss, accuracy = model.evaluate(X_test, y_test)
print('Accuracy:', accuracy)

# save model dan inference
model.save('model_sentimen.h5')

# load model
model = tf.keras.models.load_model('model_sentimen.h5')

# Inference
def predict_sentiment(text):
    # Tokenisasi teks
    sequence = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(sequence, padding='post', maxlen=int(maxlen))

    # Prediksi sentimen
    prediction = model.predict(padded)

    # Mengembalikan label sentimen
    return label_encoder.inverse_transform([np.argmax(prediction)])

# Contoh prediksi sentimen
text = 'aplikasi ini sangat bagus sekali kawan no tipu no rot'
print(predict_sentiment(text))