# -*- coding: utf-8 -*-
"""2-bedah-submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14vZ2LNeO6kdmfSNhReH-axvuIK_sLjxk

## Scraping Ulasan Aplikasi Melalui Google Play Store
"""

!pip install google-play-scraper

# Mengimpor pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store.
from google_play_scraper import app, reviews, Sort, reviews_all

# Mengambil semua ulasan dari aplikasi whatsapp
# Proses scraping mungkin memerlukan beberapa saat tergantung pada jumlah ulasan yang ada.
scrapreview = reviews_all(
    'com.shopee.id',        # ID paket aplikasi
    lang='id',             # Bahasa ulasan (default: 'en')
    country='id',          # Negara (default: 'us')
    sort=Sort.MOST_RELEVANT, # Urutan ulasan (default: Sort.MOST_RELEVANT)
    count=10            # Jumlah maksimum ulasan yang ingin diambil
)

import csv
with open('review.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Review", "Rating"])
    for review in scrapreview:
        writer.writerow([review['content'], review['score']])
print("Scraping selesai. Data tersimpan dalam file 'review.csv'")

# scrapreview

import pandas as pd

app_reviews_df = pd.DataFrame(scrapreview)
app_reviews_df.shape
app_reviews_df

df = pd.read_csv('/content/review.csv')
df

"""# DATA PREPARATION"""

import pandas as pd  # Pandas untuk manipulasi dan analisis data
pd.options.mode.chained_assignment = None  # Menonaktifkan peringatan chaining
import numpy as np  # NumPy untuk komputasi numerik
seed = 0
np.random.seed(seed)  # Mengatur seed untuk reproduktibilitas
import matplotlib.pyplot as plt  # Matplotlib untuk visualisasi data
import seaborn as sns  # Seaborn untuk visualisasi data statistik, mengatur gaya visualisasi

import datetime as dt  # Manipulasi data waktu dan tanggal
import re  # Modul untuk bekerja dengan ekspresi reguler
import string  # Berisi konstanta string, seperti tanda baca
from nltk.tokenize import word_tokenize  # Tokenisasi teks
from nltk.corpus import stopwords  # Daftar kata-kata berhenti dalam teks

!pip install sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Stemming (penghilangan imbuhan kata) dalam bahasa Indonesia
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Menghapus kata-kata berhenti dalam bahasa Indonesia

from wordcloud import WordCloud  # Membuat visualisasi berbentuk awan kata (word cloud) dari teks

import nltk  # Import pustaka NLTK (Natural Language Toolkit).
nltk.download('punkt')  # Mengunduh dataset yang diperlukan untuk tokenisasi teks.
nltk.download('stopwords')  # Mengunduh dataset yang berisi daftar kata-kata berhenti (stopwords) dalam berbagai bahasa.

# Membuat DataFrame baru (clean_df) dengan menghapus baris yang memiliki nilai yang hilang (NaN) dari app_reviews_df
clean_df = df.dropna()
clean_df

# Menghapus baris duplikat dari DataFrame clean_df
clean_df = clean_df.drop_duplicates()

# Menghitung jumlah baris dan kolom dalam DataFrame clean_df setelah menghapus duplikat
jumlah_ulasan_setelah_hapus_duplikat, jumlah_kolom_setelah_hapus_duplikat = clean_df.shape
print("Jumlah baris setelah menghapus duplikat:", jumlah_ulasan_setelah_hapus_duplikat)
print("Jumlah kolom setelah menghapus duplikat:", jumlah_kolom_setelah_hapus_duplikat)

import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag
    text = re.sub(r'RT[\s]', '', text) # menghapus RT
    text = re.sub(r"http\S+", '', text) # menghapus link
    text = re.sub(r'[0-9]+', '', text) # menghapus angka
    text = re.sub(r'[^\w\s]', '', text) # menghapus karakter selain huruf dan angka

    text = text.replace('\n', ' ') # mengganti baris baru dengan spasi
    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca
    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks
    return text

def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil
    text = text.lower()
    return text

def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token
    text = word_tokenize(text)
    return text

def filteringText(text): # Menghapus stopwords dalam teks
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy"])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata
    # Membuat objek stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()

    # Memecah teks menjadi daftar kata
    words = text.split()

    # Menerapkan stemming pada setiap kata dalam daftar
    stemmed_words = [stemmer.stem(word) for word in words]

    # Menggabungkan kata-kata yang telah distem
    stemmed_text = ' '.join(stemmed_words)

    return stemmed_text

def toSentence(list_words): # Mengubah daftar kata menjadi kalimat
    sentence = ' '.join(word for word in list_words)
    return sentence

slangwords = {"@": "di", "abis": "habis", "wtb": "beli", "masi": "masih", "wts": "jual", "wtt": "tukar", "bgt": "banget", "maks": "maksimal"}
def fix_slangwords(text):
    words = text.split()
    fixed_words = []

    for word in words:
        if word.lower() in slangwords:
            fixed_words.append(slangwords[word.lower()])
        else:
            fixed_words.append(word)

    fixed_text = ' '.join(fixed_words)
    return fixed_text

# Membersihkan teks dan menyimpannya di kolom 'text_clean'
clean_df['text_clean'] = clean_df['Review'].apply(cleaningText)

# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'
clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)

# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'
clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)

# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'
clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)

# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'
clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)

# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'
clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)

df = clean_df.head(10000)
df

"""# LABELING

Terdapat 3 contoh penglabelan :
- Berdasarkan rating
- Textblob
- VADER
- lexycon

Gunakan salah satu
"""

def label_rating(rating):
    if rating > 3:
        return 'positive'
    elif rating == 3:
        return 'neutral'
    else:
        return 'negative'

df['polarity_rating'] = df['Rating'].apply(label_rating)
df['polarity_rating'].value_counts()

from textblob import TextBlob

def analisis_sentimen(text):
    analysis = TextBlob(text)
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity == 0:
        return 'neutral'
    else:
        return 'negative'

df['polarity_textblob'] = df['text_akhir'].apply(analisis_sentimen)

nltk.download('vader_lexicon')

# Menggunakan VADER
from nltk.sentiment.vader import SentimentIntensityAnalyzer

def analisis_sentimen_vader(text):
    analyzer = SentimentIntensityAnalyzer()
    analysis = analyzer.polarity_scores(text)
    if analysis['compound'] > 0:
        return 'positive'
    elif analysis['compound'] == 0:
        return 'neutral'
    else:
        return 'negative'

df['polarity_vader'] = df['text_akhir'].apply(analisis_sentimen_vader)

import csv
import requests
from io import StringIO

# Membaca data kamus kata-kata positif dari GitHub
lexicon_positive = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub

if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma

    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_positive[row[0]] = int(row[1])
        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive
else:
    print("Failed to fetch positive lexicon data")

# Membaca data kamus kata-kata negatif dari GitHub
lexicon_negative = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub

if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma

    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_negative[row[0]] = int(row[1])
        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative
else:
    print("Failed to fetch negative lexicon data")

# Fungsi untuk menentukan polaritas sentimen dari tweet

def sentiment_analysis_lexicon_indonesia(text):
    #for word in text:

    score = 0
    # Inisialisasi skor sentimen ke 0

    for word in text:
        # Mengulangi setiap kata dalam teks

        if (word in lexicon_positive):
            score = score + lexicon_positive[word]
            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen

    for word in text:
        # Mengulangi setiap kata dalam teks (sekali lagi)

        if (word in lexicon_negative):
            score = score + lexicon_negative[word]
            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen

    polarity=''
    # Inisialisasi variabel polaritas

    # buat polarity neutral, positive, negative
    if score > 1:
        polarity = 'positive'
    elif score < -1:
        polarity = 'negative'
    else:
        polarity = 'neutral'

    return score, polarity
    # Mengembalikan skor sentimen dan polaritas teks

results = df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
df['polarity_score'] = results[0]
df['polarity'] = results[1]
print(df['polarity'].value_counts())

# value count polarity
df['polarity_textblob'].value_counts()

# value count polarity
df['polarity'].value_counts()

# value count polarity
df['polarity_vader'].value_counts()

# value count polarity
df['polarity_rating'].value_counts()

df[df['polarity_rating'] == 'positive'].head(3)

df[df['polarity_rating'] == 'negative'].head(3)

df[df['polarity_rating'] == 'neutral'].head(3)

"""# DATA SPLITTING DAN EKSTRAKSI FITUR"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Pisahkan data menjadi fitur (tweet) dan label (sentimen)
X = df['text_akhir']
# y = df['polarity']
# y = df['polarity_textblob']
y = df['polarity_vader']
# y = df['polarity_rating']


# jumlah y
unique, counts = np.unique(y, return_counts=True)
dict(zip(unique, counts))

# Encode label
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )
X_tfidf = tfidf.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Ektraksi fitur dengan BoW
from sklearn.feature_extraction.text import CountVectorizer

# Membuat objek CountVectorizer
bow = CountVectorizer(max_features=200, min_df=17, max_df=0.8)
X_bow = bow.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_bow.toarray(), columns=bow.get_feature_names_out())

# Ekstraksi fitur dengan metode GloVe
import gensim.downloader as api

# Mengunduh model GloVe
glove_model = api.load('glove-twitter-25')

# Membuat vektor fitur GloVe
X_glove = np.array([np.mean([glove_model[word] for word in document.split() if word in glove_model] or [np.zeros(25)], axis=0) for document in X])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_glove)

# Ektrasi fitur dengan metode Word2Vec
from gensim.models import Word2Vec

# Tokenisasi teks
tokenized_text = [word_tokenize(text) for text in df['text_akhir']]
# Membuat model Word2Vec
word2vec = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=1)

# Membuat vektor fitur Word2Vec
X_word2vec = np.array([np.mean([word2vec.wv[word] for word in document if word in word2vec.wv] or [np.zeros(100)], axis=0) for document in tokenized_text])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_word2vec)

# Ekstraksi fitur dengan metode FastText
from gensim.models import FastText

# Membuat model FastText
fasttext = FastText(sentences=tokenized_text, vector_size=100, window=5, min_count=1, sg=1)

# Membuat vektor fitur FastText
X_fasttext = np.array([np.mean([fasttext.wv[word] for word in document if word in fasttext.wv] or [np.zeros(100)], axis=0) for document in tokenized_text])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_fasttext)

# Ekstraksi fitur dengan metode Doc2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Membuat dokumen berlabel
tagged_data = [TaggedDocument(words=word_tokenize(text), tags=[str(i)]) for i, text in enumerate(df['text_akhir'])]

# Membuat model Doc2Vec
doc2vec = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=100)
doc2vec.build_vocab(tagged_data)
doc2vec.train(tagged_data, total_examples=doc2vec.corpus_count, epochs=doc2vec.epochs)

# Membuat vektor fitur Doc2Vec
X_doc2vec = np.array([doc2vec.infer_vector(word_tokenize(text)) for text in df['text_akhir']])

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_doc2vec)

# Menampilkan hasil ekstraksi fitur
features_df

"""Pilih salah satu text vectorization dengan mengaktifkan salah satu baris. Disini menggunakan word2vec. Silahkan berikan comment # di awal baris untuk menonaktifkan, kemudian menyalakan salah satu line

contoh :

```
# X_train, X_test, y_train, y_test = train_test_split(X_glove, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)
```

menjadi

```
X_train, X_test, y_train, y_test = train_test_split(X_glove, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)
```
"""

# Bagi data menjadi data latih dan data uji
# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_glove, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_fasttext, y, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_doc2vec, y, test_size=0.2, random_state=42)

""" # Penyeimbangan Label jika diperlukan


"""

# distribusi label
unique, counts = np.unique(y_train, return_counts=True)
dict(zip(unique, counts))

from imblearn.over_sampling import SMOTE

# Seimbangkan kelas dengan SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# distribusi label
unique, counts = np.unique(y_train, return_counts=True)
dict(zip(unique, counts))

"""# TRAINING DENGAN NAIVE BAYES

sudah kombinasi dengan ektrasi fitur yang dipilih
"""

from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score

# Membuat objek model Naive Bayes (Bernoulli Naive Bayes)
naive_bayes = BernoulliNB()

# Melatih model Naive Bayes pada data pelatihan
# naive_bayes.fit(X_train.toarray(), y_train)
naive_bayes.fit(X_train, y_train)

# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_nb = naive_bayes.predict(X_train.toarray())
# y_pred_test_nb = naive_bayes.predict(X_test.toarray())

y_pred_train_nb = naive_bayes.predict(X_train)
y_pred_test_nb = naive_bayes.predict(X_test)

# Evaluasi akurasi model Naive Bayes
accuracy_train_nb = accuracy_score(y_pred_train_nb, y_train)
accuracy_test_nb = accuracy_score(y_pred_test_nb, y_test)

# Menampilkan akurasi
print('Naive Bayes - accuracy_train:', accuracy_train_nb)
print('Naive Bayes - accuracy_test:', accuracy_test_nb)

"""# TRAINING DENGAN RANDOMFOREST

sudah kombinasi dengan ektrasi fitur yang dipilih
"""

from sklearn.ensemble import RandomForestClassifier

# Membuat objek model Random Forest
random_forest = RandomForestClassifier()

# Melatih model Random Forest pada data pelatihan
# random_forest.fit(X_train.toarray(), y_train)
random_forest.fit(X_train, y_train)

# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_rf = random_forest.predict(X_train.toarray())
# y_pred_test_rf = random_forest.predict(X_test.toarray())

y_pred_train_rf = random_forest.predict(X_train)
y_pred_test_rf = random_forest.predict(X_test)


# Evaluasi akurasi model Random Forest
accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)
accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)

# Menampilkan akurasi
print('Random Forest - accuracy_train:', accuracy_train_rf)
print('Random Forest - accuracy_test:', accuracy_test_rf)

"""# TRAINING DENGAN LOGISTIC REGRESSIoN

sudah kombinasi dengan ektrasi fitur yang dipilih
"""

from sklearn.linear_model import LogisticRegression

# Membuat objek model Logistic Regression
logistic_regression = LogisticRegression(max_iter=1000)

# Melatih model Logistic Regression pada data pelatihan
# logistic_regression.fit(X_train.toarray(), y_train)

logistic_regression.fit(X_train, y_train)

# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_lr = logistic_regression.predict(X_train.toarray())
# y_pred_test_lr = logistic_regression.predict(X_test.toarray())

y_pred_train_lr = logistic_regression.predict(X_train)
y_pred_test_lr = logistic_regression.predict(X_test)

# Evaluasi akurasi model Logistic Regression pada data pelatihan
accuracy_train_lr = accuracy_score(y_pred_train_lr, y_train)

# Evaluasi akurasi model Logistic Regression pada data uji
accuracy_test_lr = accuracy_score(y_pred_test_lr, y_test)

# Menampilkan akurasi
print('Logistic Regression - accuracy_train:', accuracy_train_lr)
print('Logistic Regression - accuracy_test:', accuracy_test_lr)

"""# TRAINING DENGAN DECISION TREE

sudah kombinasi dengan ektrasi fitur yang dipilih
"""

from sklearn.tree import DecisionTreeClassifier

# Membuat objek model Decision Tree
decision_tree = DecisionTreeClassifier()

# Melatih model Decision Tree pada data pelatihan
# decision_tree.fit(X_train.toarray(), y_train)
decision_tree.fit(X_train, y_train)


# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_dt = decision_tree.predict(X_train.toarray())
# y_pred_test_dt = decision_tree.predict(X_test.toarray())

y_pred_train_dt = decision_tree.predict(X_train)
y_pred_test_dt = decision_tree.predict(X_test)

# Evaluasi akurasi model Decision Tree
accuracy_train_dt = accuracy_score(y_pred_train_dt, y_train)
accuracy_test_dt = accuracy_score(y_pred_test_dt, y_test)

# Menampilkan akurasi
print('Decision Tree - accuracy_train:', accuracy_train_dt)
print('Decision Tree - accuracy_test:', accuracy_test_dt)

"""# TRAINING DENGAN GRADIENT BOOSTING

sudah kombinasi dengan ektrasi fitur yang dipilih
"""

# Model menggunakan gradient base
from sklearn.ensemble import GradientBoostingClassifier

# Membuat objek model Gradient Boosting
gradient_boosting = GradientBoostingClassifier()

# Melatih model Gradient Boosting pada data pelatihan
# gradient_boosting.fit(X_train.toarray(), y_train)
gradient_boosting.fit(X_train, y_train)

# Prediksi sentimen pada data pelatihan dan data uji
# y_pred_train_gb = gradient_boosting.predict(X_train.toarray())
# y_pred_test_gb = gradient_boosting.predict(X_test.toarray())

y_pred_train_gb = gradient_boosting.predict(X_train)
y_pred_test_gb = gradient_boosting.predict(X_test)

# Evaluasi akurasi model Gradient Boosting
accuracy_train_gb = accuracy_score(y_pred_train_gb, y_train)
accuracy_test_gb = accuracy_score(y_pred_test_gb, y_test)

# Menampilkan akurasi
print('Gradient Boosting - accuracy_train:', accuracy_train_gb)
print('Gradient Boosting - accuracy_test:', accuracy_test_gb)

# catboost
# !pip install catboost

from catboost import CatBoostClassifier

catboost = CatBoostClassifier()
catboost.fit(X_train, y_train)

y_pred_train_catboost = catboost.predict(X_train)
y_pred_test_catboost = catboost.predict(X_test)

accuracy_train_catboost = accuracy_score(y_pred_train_catboost, y_train)
accuracy_test_catboost = accuracy_score(y_pred_test_catboost, y_test)

print('CatBoost - accuracy_train:', accuracy_train_catboost)
print('CatBoost - accuracy_test:', accuracy_test_catboost)

"""# SIMPAN MODEL & INFERENCE"""

# simpan semua model
import joblib

joblib.dump(naive_bayes, 'naive_bayes_model.pkl')
joblib.dump(random_forest, 'random_forest_model.pkl')
joblib.dump(logistic_regression, 'logistic_regression_model.pkl')
joblib.dump(decision_tree, 'decision_tree_model.pkl')
joblib.dump(gradient_boosting, 'gradient_boosting_model.pkl')
joblib.dump(catboost, 'catboost_model.pkl')

# inference
naive_bayes = joblib.load('naive_bayes_model.pkl')
random_forest = joblib.load('random_forest_model.pkl')
logistic_regression = joblib.load('logistic_regression_model.pkl')
decision_tree = joblib.load('decision_tree_model.pkl')
gradient_boosting = joblib.load('gradient_boosting_model.pkl')
catboost = joblib.load('catboost_model.pkl')

def predict_sentiment(text):
    # Membersihkan teks
    text_clean = cleaningText(text)
    text_casefolding = casefoldingText(text_clean)
    text_slangwords = fix_slangwords(text_casefolding)
    text_tokenizing = tokenizingText(text_slangwords)
    text_stopword = filteringText(text_tokenizing)
    text_akhir = toSentence(text_stopword)

    # Ekstraksi fitur
    # text_tfidf = tfidf.transform([text_akhir])
    # text_bow = bow.transform([text_akhir])
    text_word2vec = np.array([np.mean([word2vec.wv[word] for word in word_tokenize(text_akhir) if word in word2vec.wv] or [np.zeros(100)], axis=0)
                              for document in [text_akhir]])
    # text_glove = np.array([np.mean([glove_model[word] for word in word_tokenize(text_akhir) if word in glove_model] or [np.zeros(25)], axis=0)
    #                       for document in [text_akhir]])
    # text_fasttext = np.array([np.mean([fasttext.wv[word] for word in word_tokenize(text_akhir) if word in fasttext.wv] or [np.zeros(100)], axis=0)
    #                         for document in [text_akhir]])
    # text_doc2vec = np.array([doc2vec.infer_vector(word_tokenize(text_akhir))])

    # Prediksi sentimen
    # sentiment_nb = naive_bayes.predict(text_tfidf.toarray())
    # sentiment_rf = random_forest.predict(text_tfidf.toarray())
    # sentiment_lr = logistic_regression.predict(text_tfidf.toarray())
    # sentiment_dt = decision_tree.predict(text_tfidf.toarray())

    sentiment_nb = naive_bayes.predict(text_word2vec)
    sentiment_rf = random_forest.predict(text_word2vec)
    sentiment_lr = logistic_regression.predict(text_word2vec)
    sentiment_dt = decision_tree.predict(text_word2vec)
    sentiment_gb = gradient_boosting.predict(text_word2vec)
    sentiment_catboost = catboost.predict(text_word2vec)

    return sentiment_nb, sentiment_rf, sentiment_lr, sentiment_dt, sentiment_gb, sentiment_catboost

# text = 'Aplikasi ini sangat membantu saya dalam mengelola keuangan. Sangat direkomendasikan!'
text = 'Aplikasi ini sangat buruk, tidak bisa digunakan. Saya sangat kecewa!'
sentiment_nb, sentiment_rf, sentiment_lr, sentiment_dt, sentiment_gb, sentiment_catboost = predict_sentiment(text)

print('Naive Bayes:', label_encoder.inverse_transform(sentiment_nb))
print('Random Forest:', label_encoder.inverse_transform(sentiment_rf))
print('Logistic Regression:', label_encoder.inverse_transform(sentiment_lr))
print('Decision Tree:', label_encoder.inverse_transform(sentiment_dt))
print('Gradient Boosting:', label_encoder.inverse_transform(sentiment_gb))
print('CatBoost:', label_encoder.inverse_transform(sentiment_catboost))

# print('Naive Bayes:', sentiment_nb)
# print('Random Forest:', sentiment_rf)
# print('Logistic Regression:', sentiment_lr)
# print('Decision Tree:', sentiment_dt)

"""# TRAINING DENGAN NEURAL NETWORK SEDERHANA

Sudah kombinasi ekstraksi fitur
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib
import numpy as np

# Pisahkan data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X_glove, y, test_size=0.2, random_state=42)

# Encode label
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

joblib.dump(label_encoder, 'label_encoder.pkl')

# Distribusi label
unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

# Membuat model Dense
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))  # input_shape sesuai dengan dimensi data
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))  # 3 kelas

# Kompilasi model
model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Membuat objek EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

# Melatih model
history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32, callbacks=[early_stopping])

# Evaluasi model
_, accuracy = model.evaluate(X_test, y_test)
print('Accuracy:', accuracy)

# Simpan model
model.save('sentiment_nn_model.h5')

# Inference
from tensorflow.keras.models import load_model

model = load_model('sentiment_nn_model.h5')
label_encoder = joblib.load('label_encoder.pkl')

def predict_sentiment_nn(text):
    # Membersihkan teks
    text_clean = cleaningText(text)
    text_casefolding = casefoldingText(text_clean)
    text_slangwords = fix_slangwords(text_casefolding)
    text_tokenizing = tokenizingText(text_slangwords)
    text_stopword = filteringText(text_tokenizing)
    text_akhir = toSentence(text_stopword)

    # Ekstraksi fitur
    text_glove = np.array([np.mean([glove_model[word] for word in word_tokenize(text_akhir) if word in glove_model] or [np.zeros(25)], axis=0)
                          for document in [text_akhir]])

    # Prediksi sentimen
    sentiment_probs = model.predict(text_glove)
    sentiment_nn = np.argmax(sentiment_probs, axis=1)

    return sentiment_nn, sentiment_probs

text = 'Aplikasi ini sangat buruk, tidak bisa digunakan. Saya sangat kecewa!'
sentiment_nn, sentiment_probs = predict_sentiment_nn(text)
print(sentiment_probs)
print('Neural Network:', label_encoder.inverse_transform(sentiment_nn))

"""# LSTM"""

X = df['text_akhir'].values
y = df['polarity_vader'].values
length = df['text_akhir'].str.len().max()
print(length)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Buat instance LabelEncoder
label_encoder = LabelEncoder()

# Fit dan transform label_train
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Simpan label encoder untuk penggunaan di masa depan
import joblib
joblib.dump(label_encoder, 'label_encoder.pkl')

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = length, oov_token = '')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sequences_train,
                             maxlen = 5,
                             padding = 'post',
                             truncating = 'post')
padded_test = pad_sequences(sequences_test,
                            maxlen = 5,
                            padding = 'post',
                            truncating = 'post')

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(254, activation='relu'),
    tf.keras.layers.Dense(128, activation= 'relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])


model.compile(loss ='sparse_categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

#Callback Function
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping

class accCallback(Callback):
   def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.98 and logs.get('val_accuracy') >= 0.98):
            print("\nAccuracy and Val_Accuracy has reached 90%!", "\nEpoch: ", epoch)
            self.model.stop_training = True

callbacks = accCallback()

auto_reduction_LR = ReduceLROnPlateau(
    monitor = 'val_accuracy',
    patience = 2, #setelah 2 epoch, jika tidak ada kenaikan maka LR berkurang
    verbose = 1,
    factor = 0.2,
    min_lr = 0.000003
)

auto_stop_learn = EarlyStopping(
    monitor = 'val_accuracy',
    min_delta = 0,
    patience = 4,
    verbose = 1,
    mode = 'auto'
)

#latih model
history = model.fit(padded_train, y_train,
                    epochs = 100,
                    validation_data = (padded_test, y_test),
                    verbose = 1,
                    callbacks=[callbacks, auto_reduction_LR, auto_stop_learn],
                    )

# Evaluasi model
_, accuracy = model.evaluate(padded_test, y_test)
print('Accuracy:', accuracy)

model.save('sentiment_lstm_model2.h5')

# inference model lstm skema 2
from tensorflow.keras.models import load_model

model = load_model('sentiment_lstm_model2.h5')
label_encoder = joblib.load('label_encoder.pkl')

def predict_sentiment_lstm(text):
    # Membersihkan teks
    text_clean = cleaningText(text)
    text_casefolding = casefoldingText(text_clean)
    text_slangwords = fix_slangwords(text_casefolding)
    text_tokenizing = tokenizingText(text_slangwords)
    text_stopword = filteringText(text_tokenizing)
    text_akhir = toSentence(text_stopword)

    # Tokenisasi teks
    sequences = tokenizer.texts_to_sequences([text_akhir])
    padded = pad_sequences(sequences, maxlen=5, padding='post', truncating='post')

    # Prediksi sentimen
    sentiment_probs = model.predict(padded)
    sentiment_lstm = np.argmax(sentiment_probs, axis=1)

    return sentiment_lstm, sentiment_probs

text = 'Aplikasi ini sangat buruk, tidak bisa digunakan. Saya sangat kecewa!'
sentiment_lstm, sentiment_probs = predict_sentiment_lstm(text)
print(sentiment_probs)
print('LSTM:', label_encoder.inverse_transform(sentiment_lstm))

"""# TRAINING DENGAN GRU

Sudah kombinasi ekstraksi fitur
"""

# TRAINING MODEL MENGGUNAKAN GRU
from tensorflow.keras.layers import GRU

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=32),
    tf.keras.layers.GRU(64),
    tf.keras.layers.Dense(254, activation='relu'),
    tf.keras.layers.Dense(128, activation= 'relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(loss ='sparse_categorical_crossentropy',
                optimizer = 'adam',
                metrics = ['accuracy'])

#Callback Function
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping

class accCallback (Callback):
    def on_epoch_end(self, epoch, logs={}):
          if(logs.get('accuracy') >= 0.92 and logs.get('val_accuracy') >= 0.92):
                print("\nAccuracy and Val_Accuracy has reached 90%!", "\nEpoch: ", epoch)
                self.model.stop_training = True

callbacks = accCallback()

auto_reduction_LR = ReduceLROnPlateau(
    monitor = 'val_accuracy',
    patience = 2, #setelah 2 epoch, jika tidak ada kenaikan maka LR berkurang
    verbose = 1,
    factor = 0.2,
    min_lr = 0.000003
)

auto_stop_learn = EarlyStopping(
    monitor = 'val_accuracy',
    min_delta = 0,
    patience = 4,
    verbose = 1,
    mode = 'auto'
)

#latih model
history = model.fit(padded_train, y_train,
                    epochs = 100,
                    validation_data = (padded_test, y_test),
                    verbose = 1,
                    callbacks=[callbacks, auto_reduction_LR, auto_stop_learn],
                    )


model.save('sentiment_gru_model.h5')

# inference model gru
from tensorflow.keras.models import load_model

model = load_model('sentiment_gru_model.h5')
label_encoder = joblib.load('label_encoder.pkl')

def predict_sentiment_gru(text):
    # Membersihkan teks
    text_clean = cleaningText(text)
    text_casefolding = casefoldingText(text_clean)
    text_slangwords = fix_slangwords(text_casefolding)
    text_tokenizing = tokenizingText(text_slangwords)
    text_stopword = filteringText(text_tokenizing)
    text_akhir = toSentence(text_stopword)

    # Tokenisasi teks
    sequences = tokenizer.texts_to_sequences([text_akhir])
    padded = pad_sequences(sequences, maxlen=5, padding='post', truncating='post')

    # Prediksi sentimen
    sentiment_probs = model.predict(padded)
    sentiment_gru = np.argmax(sentiment_probs, axis=1)

    return sentiment_gru, sentiment_probs

text = 'Aplikasi ini sangat buruk, tidak bisa digunakan. Saya sangat kecewa!'
sentiment_gru, sentiment_probs = predict_sentiment_gru(text)
print(sentiment_probs)
print('GRU:', label_encoder.inverse_transform(sentiment_gru))