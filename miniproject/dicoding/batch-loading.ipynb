{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akjiktQZuXfh"
      },
      "source": [
        "<h1> Penggunaan Batch Loading </h1>\n",
        "\n",
        "Batch loading adalah proses pelatihan dimana jst melakukan pembaruan parameternya(weight) setelah membaca sejumlah sampel data tertentu. Misal dataset kita berisi 800 buah gambar pizza. Tanpa batch size, proses pembaruan parameter terjadi untuk selutuh sampel pada dataset. Sehingga ketika tanpa menggunakan batch size, pada 1 epoch terdapat 800 kali pembaruan weight. Ketika 1 ukuran batch adalah 32 buah gambar pizza, maka terdapat 25 batch pada dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qqXV8JG6uPEU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "   49152/11490434 [..............................] - ETA: 10:42"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m mnist \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mmnist\n\u001b[0;32m      5\u001b[0m \u001b[39m#pisahkan\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m (training_images, training_labels), (test_images, test_labels) \u001b[39m=\u001b[39m mnist\u001b[39m.\u001b[39;49mload_data()\n\u001b[0;32m      7\u001b[0m training_images \u001b[39m=\u001b[39m training_images \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[0;32m      8\u001b[0m test_images \u001b[39m=\u001b[39m test_images \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\datasets\\mnist.py:75\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads the MNIST dataset.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39mThis is a dataset of 60,000 28x28 grayscale images of the 10 digits,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m  https://creativecommons.org/licenses/by-sa/3.0/)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m origin_folder \u001b[39m=\u001b[39m (\n\u001b[0;32m     73\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://storage.googleapis.com/tensorflow/tf-keras-datasets/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m )\n\u001b[1;32m---> 75\u001b[0m path \u001b[39m=\u001b[39m get_file(\n\u001b[0;32m     76\u001b[0m     path,\n\u001b[0;32m     77\u001b[0m     origin\u001b[39m=\u001b[39;49morigin_folder \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmnist.npz\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     78\u001b[0m     file_hash\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39mload(path, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     81\u001b[0m     x_train, y_train \u001b[39m=\u001b[39m f[\u001b[39m\"\u001b[39m\u001b[39mx_train\u001b[39m\u001b[39m\"\u001b[39m], f[\u001b[39m\"\u001b[39m\u001b[39my_train\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:296\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m         urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    297\u001b[0m     \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    298\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39mcode, e\u001b[39m.\u001b[39mmsg))\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:86\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     84\u001b[0m response \u001b[39m=\u001b[39m urlopen(url, data)\n\u001b[0;32m     85\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fd:\n\u001b[1;32m---> 86\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunk_read(response, reporthook\u001b[39m=\u001b[39mreporthook):\n\u001b[0;32m     87\u001b[0m         fd\u001b[39m.\u001b[39mwrite(chunk)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:75\u001b[0m, in \u001b[0;36murlretrieve.<locals>.chunk_read\u001b[1;34m(response, chunk_size, reporthook)\u001b[0m\n\u001b[0;32m     73\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     chunk \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mread(chunk_size)\n\u001b[0;32m     76\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m reporthook \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#kita memakai dataset mnist\n",
        "mnist = tf.keras.datasets.mnist\n",
        "#pisahkan\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "#buat model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (28, 28)),\n",
        "                                    tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation = tf.nn.softmax)])\n",
        "#compile\n",
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj98nBr9wbpn"
      },
      "source": [
        "Disini kita mulai menggunakan batch loading. Untuk menggunakan batch loading kita hanya tinggal menambahkan parameter ‘batch_size’ pada fungsi fit(). Tahukah Anda bahwa fungsi fit() secara default menggunakan batch loading dengan batch size sebesar 32. Lantas  ketika kita tidak mendefinisikan parameter batch_size, maka ukuran batch akan diisi sebesar 32 secara default. Perhatikan bahwa pada setiap epoch memakan waktu selama sekitar 3 atau 4 detik."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMLgGvh6wpMw",
        "outputId": "73254eb0-2657-4448-8db9-54c6a2d7080d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2601 - accuracy: 0.9245\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1172 - accuracy: 0.9652\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0810 - accuracy: 0.9762\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0602 - accuracy: 0.9815\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0471 - accuracy: 0.9859\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa3e0d3af90>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(training_images, training_labels, batch_size = 32, epochs = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLrvOO2iwwfU"
      },
      "source": [
        "Selanjutnya kita akan menggunakan batch_size yang lebih besar yaitu 128. Dapat kita lihat bahwa semakin besar batch size, waktu eksekusi tiap epoch akan semakin cepat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojGjqfDSwuu1",
        "outputId": "fe2b4b56-38f3-4c01-9487-84ad376a4c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0254 - accuracy: 0.9934\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0212 - accuracy: 0.9952\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0188 - accuracy: 0.9958\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0167 - accuracy: 0.9966\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0150 - accuracy: 0.9971\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa3dd5bc4d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(training_images, training_labels, batch_size=128, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re5t07dAxnHp"
      },
      "source": [
        "Pada latihan ini kita telah memahami bahwa dengan menggunakan batch loading, kita dapat mempercepat pelatihan model kita. Untuk pemilihan batch size sendiri tidak ada aturan bakunya namun yang umum dipakai adalah 32,64, dan 128. Anda umumnya harus bereksperimen sendiri guna menemukan batch size yang cocok dengan masalah Anda. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "9429e3d8733c1ac76f5c3b0c6cdd2706acd1fd7b4f8c91f27a242412c8600fc4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
